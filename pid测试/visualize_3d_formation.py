#!/usr/bin/env python3
# visualize_3d_formation.py - å®Œå…¨ä¿®å¤ç‰ˆå¯è§†åŒ–å·¥å…·

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from rl_env_f16_formation import FormationEnvFixed
from config import TRAIN_CONFIG_FIXED
import os


class PPOVisualizer:
    """
    PPOè®­ç»ƒç»“æœå¯è§†åŒ–å™¨ - å®Œå…¨ä¿®å¤ç‰ˆ

    ä¿®å¤å†…å®¹:
    1. æ­£ç¡®è®¿é—®VecNormalizeåŒ…è£…çš„ç¯å¢ƒ
    2. è®°å½•ä¸‰ç±»è¯¯å·®: Îµâ‚(çœŸå®), Îµâ‚‚(è§„åˆ’), Îµâ‚ƒ(æ§åˆ¶)
    3. åˆ†ç¦»æ°´å¹³/é«˜åº¦è¯¯å·®
    4. æ—¶é—´å¯¹é½å’Œæ•°æ®ç»´åº¦ä¿æŠ¤
    """

    def __init__(self, model_path, vec_normalize_path=None):
        """
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚: "./logs/ppo_fixed/best_model")
            vec_normalize_path: VecNormalizeç»Ÿè®¡æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: "./logs/ppo_fixed/vec_normalize.pkl")
        """
        self.model_path = model_path
        self.vec_normalize_path = vec_normalize_path

        # åŠ è½½ç¯å¢ƒé…ç½®
        config = TRAIN_CONFIG_FIXED.to_dict()
        self.config = config

        # åˆ›å»ºç¯å¢ƒ
        env = DummyVecEnv([lambda: FormationEnvFixed(config)])

        # åŠ è½½å½’ä¸€åŒ–å‚æ•°
        if vec_normalize_path and os.path.exists(vec_normalize_path):
            env = VecNormalize.load(vec_normalize_path, env)
            env.training = False
            env.norm_reward = False
            print(f"âœ… Loaded VecNormalize from {vec_normalize_path}")
        else:
            print("âš ï¸  No VecNormalize file found, using raw observations")

        # åŠ è½½æ¨¡å‹
        self.model = PPO.load(model_path, env=env)
        self.env = env

        # ğŸ”¥ ä¿®å¤: æ­£ç¡®è·å–åº•å±‚ç¯å¢ƒ
        if isinstance(env, VecNormalize):
            self.base_env = env.venv.envs[0]
        else:
            self.base_env = env.envs[0]

        print(f"âœ… Loaded model from {model_path}")

    def evaluate_episode(self, seed=42, render_mode='full'):
        """
        è¯„ä¼°å•ä¸ªepisodeå¹¶æ”¶é›†æ•°æ®

        Args:
            seed: éšæœºç§å­
            render_mode: 'full' æˆ– 'compact'

        Returns:
            history: åŒ…å«å®Œæ•´è½¨è¿¹æ•°æ®çš„å­—å…¸
        """
        obs = self.env.reset()
        if hasattr(self.env, 'seed'):
            self.env.seed(seed)

        history = {
            'time': [],
            'positions': [[] for _ in range(4)],
            'target_positions': [[] for _ in range(4)],
            'ideal_positions': [[] for _ in range(4)],  # ğŸ”¥ æ–°å¢: ç†æƒ³ç¼–é˜Ÿä½ç½®

            # ğŸ”¥ ä¿®å¤: ä¸‰ç±»è¯¯å·®
            'error_total': [[] for _ in range(4)],  # Îµâ‚: Agent vs ç†æƒ³ç¼–é˜Ÿ
            'error_planning': [[] for _ in range(4)],  # Îµâ‚‚: åå•†è½¨è¿¹ vs ç†æƒ³ç¼–é˜Ÿ
            'error_tracking': [[] for _ in range(4)],  # Îµâ‚ƒ: Agent vs åå•†è½¨è¿¹

            # ğŸ”¥ æ–°å¢: åˆ†ç»´åº¦è¯¯å·®
            'error_horizontal': [[] for _ in range(4)],
            'error_vertical': [[] for _ in range(4)],

            'rewards': [],
            'r_track_h': [],
            'r_track_v': [],
            'r_safe': [],
            'r_ctrl': [],
            'r_smooth': [],
            'min_distance': [],
            'rl_active': [],
            'actions': [],
            'leader_pos': []
        }

        done = False
        step_count = 0
        dt = 0.05

        print("\n" + "=" * 70)
        print("Running Episode Evaluation...")
        print("=" * 70)

        while not done:
            # è·å–åŠ¨ä½œ
            action, _ = self.model.predict(obs, deterministic=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = self.env.step(action)
            if done[0] and len(info) > 0 and isinstance(info[0], dict):
                terminal_obs = info[0].get('terminal_observation')
                if terminal_obs is not None:
                    obs = terminal_obs

            # ğŸ”¥ ä¿®å¤: æ­£ç¡®è®¿é—®åº•å±‚ç¯å¢ƒçŠ¶æ€
            env_state = self.base_env

            # è®°å½•æ•°æ®
            t = step_count * dt
            history['time'].append(t)
            history['rewards'].append(reward[0])
            history['actions'].append(action[0].copy())

            # ğŸ”¥ è·å–å½“å‰æ—‹è½¬çŸ©é˜µå’Œç¼–é˜Ÿåç§»
            c, s = np.cos(env_state.leader_heading), np.sin(env_state.leader_heading)
            R_z = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])
            current_offsets = [R_z @ off for off in env_state.desired_offsets]

            # è®°å½•å„æ™ºèƒ½ä½“ä½ç½®å’Œè¯¯å·®
            for i in range(4):
                agent = env_state.agents[i]
                history['positions'][i].append(agent.position.copy())

                # è·å–åå•†è½¨è¿¹ç›®æ ‡ä½ç½®
                target_pos_all, _ = env_state.planner.get_target_trajectories()
                history['target_positions'][i].append(target_pos_all[i].copy())

                # ğŸ”¥ è®¡ç®—ç†æƒ³ç¼–é˜Ÿä½ç½®
                ideal_pos = env_state.leader_pos + current_offsets[i]
                history['ideal_positions'][i].append(ideal_pos.copy())

                # ğŸ”¥ è®¡ç®—ä¸‰ç±»è¯¯å·®
                e_total = agent.position - ideal_pos
                error_total = np.linalg.norm(e_total)
                history['error_total'][i].append(error_total)

                error_planning = np.linalg.norm(target_pos_all[i] - ideal_pos)
                history['error_planning'][i].append(error_planning)

                error_tracking = np.linalg.norm(agent.position - target_pos_all[i])
                history['error_tracking'][i].append(error_tracking)

                # ğŸ”¥ åˆ†ç»´åº¦è¯¯å·®
                error_horizontal = np.linalg.norm(e_total[0:2])
                error_vertical = abs(e_total[2])
                history['error_horizontal'][i].append(error_horizontal)
                history['error_vertical'][i].append(error_vertical)

            # è®°å½•leaderä½ç½®
            history['leader_pos'].append(env_state.leader_pos.copy())

            # ğŸ”¥ ä¿®å¤: æå–infoä¸­çš„å¥–åŠ±åˆ†è§£ï¼ˆå¸¦é˜²å¾¡æ€§æ£€æŸ¥ï¼‰
            if len(info) > 0 and isinstance(info[0], dict):
                info_dict = info[0]
                history['r_track_h'].append(info_dict.get('r_track_h', 0))
                history['r_track_v'].append(info_dict.get('r_track_v', 0))
                history['r_safe'].append(info_dict.get('r_safe', 0))
                history['r_ctrl'].append(info_dict.get('r_ctrl', 0))
                history['r_smooth'].append(info_dict.get('r_smooth', 0))
                history['min_distance'].append(info_dict.get('min_distance', 0))
                history['rl_active'].append(info_dict.get('rl_active', False))
            else:
                # Warmstarté˜¶æ®µæˆ–ä¿¡æ¯ç¼ºå¤±
                history['r_track_h'].append(0)
                history['r_track_v'].append(0)
                history['r_safe'].append(0)
                history['r_ctrl'].append(0)
                history['r_smooth'].append(0)
                history['min_distance'].append(500.0)
                history['rl_active'].append(False)

            step_count += 1

            # è¿›åº¦æ˜¾ç¤º
            if step_count % 400 == 0:
                avg_err_total = np.mean([history['error_total'][i][-1] for i in range(4)])
                avg_err_h = np.mean([history['error_horizontal'][i][-1] for i in range(4)])
                avg_err_v = np.mean([history['error_vertical'][i][-1] for i in range(4)])
                min_dist = history['min_distance'][-1] if history['min_distance'] else 0
                print(f"t={t:.1f}s | Total={avg_err_total:.0f}ft | H={avg_err_h:.0f}ft | "
                      f"V={avg_err_v:.0f}ft | MinDist={min_dist:.0f}ft | Reward={reward[0]:.2f}")

            if done[0]:
                break

        print("\n" + "=" * 70)
        print("Episode Complete!")
        print("=" * 70)

        # ç»Ÿè®¡ä¿¡æ¯
        final_errors_total = [history['error_total'][i][-1] for i in range(4)]
        final_errors_h = [history['error_horizontal'][i][-1] for i in range(4)]
        final_errors_v = [history['error_vertical'][i][-1] for i in range(4)]
        min_distance_ever = min(history['min_distance']) if history['min_distance'] else 0

        print(f"Total Steps: {step_count}")
        print(f"Total Time: {step_count * dt:.1f}s")
        print(f"\nFinal Errors (Total): {[f'{e:.0f}ft' for e in final_errors_total]}")
        print(f"Average Final Error (Total): {np.mean(final_errors_total):.0f}ft")
        print(f"Average Final Error (Horizontal): {np.mean(final_errors_h):.0f}ft")
        print(f"Average Final Error (Vertical): {np.mean(final_errors_v):.0f}ft")
        print(f"Minimum Distance Ever: {min_distance_ever:.1f}ft")
        print(f"Total Reward: {sum(history['rewards']):.1f}")
        print("=" * 70)

        return history

    def plot_comprehensive_analysis(self, history, save_path='ppo_analysis_fixed.png'):
        """
        ç»˜åˆ¶ç»¼åˆåˆ†æå›¾ - ä¿®å¤ç‰ˆ

        åŒ…å«:
        1. 3Dè½¨è¿¹å›¾
        2. ä¸‰ç±»è¯¯å·®å¯¹æ¯”
        3. æ°´å¹³/é«˜åº¦è¯¯å·®åˆ†è§£
        4. æœ€å°è·ç¦»
        5. å¥–åŠ±åˆ†è§£
        6. RLæ¿€æ´»çŠ¶æ€
        """
        history = self._trim_history(history)
        fig = plt.figure(figsize=(24, 14))

        colors = ['red', 'green', 'blue', 'orange']
        labels = ['Agent 1 (Leader)', 'Agent 2', 'Agent 3', 'Agent 4']

        # ==================== 1. 3Dè½¨è¿¹å›¾ ====================
        ax1 = fig.add_subplot(2, 4, 1, projection='3d')

        for i in range(4):
            pos_array = np.array(history['positions'][i])
            ax1.plot(pos_array[:, 0], pos_array[:, 1], pos_array[:, 2],
                     color=colors[i], label=labels[i], linewidth=2, alpha=0.8)

        leader_array = np.array(history['leader_pos'])
        ax1.plot(leader_array[:, 0], leader_array[:, 1], leader_array[:, 2],
                 'k--', linewidth=2, label='Leader Reference', alpha=0.6)

        ax1.set_xlabel('X (North, ft)')
        ax1.set_ylabel('Y (East, ft)')
        ax1.set_zlabel('Z (Down, ft)')
        ax1.set_title('3D Flight Trajectories')
        ax1.legend(fontsize=7)
        ax1.grid(True, alpha=0.3)

        # ==================== 2. ä¸‰ç±»è¯¯å·®å¯¹æ¯” (Agent 1) ====================
        ax2 = fig.add_subplot(2, 4, 2)

        i = 0  # åªæ˜¾ç¤ºAgent 1ä½œä¸ºç¤ºä¾‹
        ax2.plot(history['time'], history['error_total'][i],
                 'r-', linewidth=2, label=f'Îµâ‚: Total (çœŸå®è·Ÿè¸ª)')
        ax2.plot(history['time'], history['error_planning'][i],
                 'b--', linewidth=1.5, label=f'Îµâ‚‚: Planning (è½¨è¿¹é‡æ„)')
        ax2.plot(history['time'], history['error_tracking'][i],
                 'g:', linewidth=1.5, label=f'Îµâ‚ƒ: Tracking (æ§åˆ¶å™¨)')

        ax2.axvspan(20, 70, alpha=0.15, color='gray', label='Turn Phase')
        ax2.set_xlabel('Time (s)')
        ax2.set_ylabel('Error (ft)')
        ax2.set_title(f'Three Error Types - {labels[0]}')
        ax2.legend(fontsize=7)
        ax2.grid(True, alpha=0.3)

        # ==================== 3. æ€»ä½“çœŸå®è·Ÿè¸ªè¯¯å·® (æ‰€æœ‰Agent) ====================
        ax3 = fig.add_subplot(2, 4, 3)

        for i in range(4):
            ax3.plot(history['time'], history['error_total'][i],
                     color=colors[i], label=labels[i], linewidth=1.5)

        ax3.axhline(
            y=self.config.get('rl_threshold', 150.0),
            color='orange',
            linestyle='--',
            linewidth=1,
            label='RL Threshold'
        )
        ax3.axvspan(20, 70, alpha=0.15, color='gray')
        ax3.set_xlabel('Time (s)')
        ax3.set_ylabel('Error (ft)')
        ax3.set_title('Îµâ‚: True Tracking Error (All Agents)')
        ax3.legend(fontsize=7)
        ax3.grid(True, alpha=0.3)

        # ==================== 4. æ°´å¹³/é«˜åº¦è¯¯å·®åˆ†è§£ ====================
        ax4 = fig.add_subplot(2, 4, 4)

        avg_error_h = [np.mean([history['error_horizontal'][i][t] for i in range(4)])
                       for t in range(len(history['time']))]
        avg_error_v = [np.mean([history['error_vertical'][i][t] for i in range(4)])
                       for t in range(len(history['time']))]

        ax4.plot(history['time'], avg_error_h, 'b-', linewidth=2, label='Horizontal Error')
        ax4.plot(history['time'], avg_error_v, 'r-', linewidth=2, label='Vertical Error')
        ax4.axvspan(20, 70, alpha=0.15, color='gray', label='Turn Phase')

        ax4.set_xlabel('Time (s)')
        ax4.set_ylabel('Error (ft)')
        ax4.set_title('Horizontal vs Vertical Error (Average)')
        ax4.legend(fontsize=8)
        ax4.grid(True, alpha=0.3)

        # ==================== 5. æœ€å°è·ç¦» ====================
        ax5 = fig.add_subplot(2, 4, 5)

        ax5.plot(history['time'], history['min_distance'], 'b-', linewidth=2)
        ax5.axhspan(0, 100, alpha=0.3, color='red', label='Collision Zone (<100ft)')
        ax5.axhspan(100, 160, alpha=0.3, color='yellow', label='Danger Zone (100-160ft)')
        ax5.axhspan(160, 350, alpha=0.15, color='orange', label='Warning Zone (160-350ft)')
        safety_margin = self.config.get('distance_safety_margin', 300.0)
        ax5.axhline(
            y=safety_margin,
            color='purple',
            linestyle='--',
            linewidth=1.5,
            label=f'Safety Margin ({safety_margin:.0f}ft)'
        )

        ax5.set_xlabel('Time (s)')
        ax5.set_ylabel('Minimum Inter-Agent Distance (ft)')
        ax5.set_title('Safety: Minimum Distance')
        ax5.legend(fontsize=7, loc='lower right')
        ax5.grid(True, alpha=0.3)

        # ==================== 6. å¥–åŠ±åˆ†è§£ ====================
        ax6 = fig.add_subplot(2, 4, 6)

        ax6.plot(history['time'], history['r_track_h'], 'b-', label='Track (H)', alpha=0.7)
        ax6.plot(history['time'], history['r_track_v'], 'c-', label='Track (V)', alpha=0.7)
        ax6.plot(history['time'], history['r_safe'], 'g-', label='Safety', alpha=0.7)
        ax6.plot(history['time'], history['r_ctrl'], 'r-', label='Control', alpha=0.7)
        ax6.plot(history['time'], history['r_smooth'], 'm-', label='Smoothness', alpha=0.7)
        ax6.plot(history['time'], history['rewards'], 'k-', label='Total', linewidth=2)

        ax6.set_xlabel('Time (s)')
        ax6.set_ylabel('Reward')
        ax6.set_title('Reward Decomposition (Fixed)')
        ax6.legend(fontsize=7)
        ax6.grid(True, alpha=0.3)

        # ==================== 7. RLæ¿€æ´»çŠ¶æ€ ====================
        ax7 = fig.add_subplot(2, 4, 7)

        rl_active_int = [1 if x else 0 for x in history['rl_active']]
        ax7.fill_between(history['time'], 0, rl_active_int, alpha=0.5, color='cyan', label='RL Active')

        ax7.set_xlabel('Time (s)')
        ax7.set_ylabel('RL Status')
        ax7.set_ylim(-0.1, 1.1)
        ax7.set_yticks([0, 1])
        ax7.set_yticklabels(['PID Only', 'PID+RL'])
        ax7.set_title('RL Activation Status')
        ax7.legend()
        ax7.grid(True, alpha=0.3)

        # ==================== 8. åŠ¨ä½œå¹…åº¦ ====================
        ax8 = fig.add_subplot(2, 4, 8)

        if len(history['actions']) > 0:
            actions_array = np.array(history['actions'])
            action_norms = np.linalg.norm(actions_array, axis=1)
            ax8.plot(history['time'], action_norms, 'purple', linewidth=1.5)

        ax8.set_xlabel('Time (s)')
        ax8.set_ylabel('Action L2 Norm')
        ax8.set_title('Control Action Magnitude')
        ax8.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=200, bbox_inches='tight')
        print(f"\nâœ… Comprehensive analysis saved to '{save_path}'")
        plt.show()

    def plot_error_decomposition(self, history, save_path='ppo_error_decomposition.png'):
        """
        ğŸ”¥ æ–°å¢: è¯¯å·®åˆ†è§£è¯¦ç»†å›¾

        ä¸ºæ¯ä¸ªAgentå•ç‹¬æ˜¾ç¤ºä¸‰ç±»è¯¯å·®
        """
        history = self._trim_history(history)
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        axes = axes.flatten()

        colors_err = {'total': 'red', 'planning': 'blue', 'tracking': 'green'}
        labels_agent = ['Agent 1 (Leader)', 'Agent 2', 'Agent 3', 'Agent 4']

        for i in range(4):
            ax = axes[i]

            ax.plot(history['time'], history['error_total'][i],
                    color=colors_err['total'], linewidth=2,
                    label='Îµâ‚: Total (çœŸå®è·Ÿè¸ª)')
            ax.plot(history['time'], history['error_planning'][i],
                    color=colors_err['planning'], linewidth=1.5, linestyle='--',
                    label='Îµâ‚‚: Planning (è½¨è¿¹é‡æ„)')
            ax.plot(history['time'], history['error_tracking'][i],
                    color=colors_err['tracking'], linewidth=1.5, linestyle=':',
                    label='Îµâ‚ƒ: Tracking (æ§åˆ¶å™¨)')

            ax.axvspan(20, 70, alpha=0.15, color='gray', label='Turn Phase')
            ax.set_xlabel('Time (s)')
            ax.set_ylabel('Error (ft)')
            ax.set_title(f'{labels_agent[i]} - Error Decomposition')
            ax.legend(fontsize=8)
            ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=200, bbox_inches='tight')
        print(f"âœ… Error decomposition saved to '{save_path}'")
        plt.show()

    def plot_top_view_comparison(self, history, save_path='ppo_topview_fixed.png'):
        """
        ç»˜åˆ¶ä¿¯è§†å›¾å¯¹æ¯” - ä¿®å¤ç‰ˆ

        å·¦å›¾: å®é™…è½¨è¿¹ vs ç†æƒ³ç¼–é˜Ÿ
        å³å›¾: å®é™…è½¨è¿¹ vs åå•†è½¨è¿¹ vs ç†æƒ³ç¼–é˜Ÿ
        """
        history = self._trim_history(history)
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))

        colors = ['red', 'green', 'blue', 'orange']
        labels = ['Agent 1', 'Agent 2', 'Agent 3', 'Agent 4']

        # ==================== å·¦å›¾: å®é™… vs ç†æƒ³ ====================
        for i in range(4):
            pos_array = np.array(history['positions'][i])
            ideal_array = np.array(history['ideal_positions'][i])

            ax1.plot(pos_array[:, 0], pos_array[:, 1],
                     color=colors[i], label=f'{labels[i]} (Actual)',
                     linewidth=2, alpha=0.8)
            ax1.plot(ideal_array[:, 0], ideal_array[:, 1],
                     color=colors[i], linestyle=':',
                     label=f'{labels[i]} (Ideal)',
                     linewidth=1, alpha=0.5)

        leader_array = np.array(history['leader_pos'])
        ax1.plot(leader_array[:, 0], leader_array[:, 1],
                 'k--', linewidth=2, label='Leader Ref', alpha=0.6)

        ax1.set_xlabel('X (North, ft)')
        ax1.set_ylabel('Y (East, ft)')
        ax1.set_title('Actual vs Ideal Formation')
        ax1.axis('equal')
        ax1.legend(fontsize=7, ncol=2)
        ax1.grid(True, alpha=0.3)

        # ==================== å³å›¾: å®é™… vs åå•† vs ç†æƒ³ ====================
        for i in range(4):
            pos_array = np.array(history['positions'][i])
            target_array = np.array(history['target_positions'][i])
            ideal_array = np.array(history['ideal_positions'][i])

            ax2.plot(ideal_array[:, 0], ideal_array[:, 1],
                     color=colors[i], linestyle=':',
                     linewidth=1, alpha=0.3, label=f'{labels[i]} (Ideal)')
            ax2.plot(target_array[:, 0], target_array[:, 1],
                     color=colors[i], linestyle='--',
                     linewidth=1.5, alpha=0.6, label=f'{labels[i]} (Negotiated)')
            ax2.plot(pos_array[:, 0], pos_array[:, 1],
                     color=colors[i], linewidth=2,
                     alpha=0.8, label=f'{labels[i]} (Actual)')

        ax2.set_xlabel('X (North, ft)')
        ax2.set_ylabel('Y (East, ft)')
        ax2.set_title('Actual vs Negotiated vs Ideal')
        ax2.axis('equal')
        ax2.legend(fontsize=6, ncol=3)
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=200, bbox_inches='tight')
        print(f"âœ… Top-view comparison saved to '{save_path}'")
        plt.show()

    @staticmethod
    def _trim_history(history):
        if not history.get('time'):
            return history
        n_steps = len(history['time'])
        trimmed = {}
        for key, value in history.items():
            if isinstance(value, list):
                if len(value) == n_steps + 1:
                    trimmed[key] = value[:n_steps]
                elif len(value) > n_steps:
                    trimmed[key] = value[:n_steps]
                else:
                    trimmed[key] = value
            else:
                trimmed[key] = value
        return trimmed


def main():
    """ä¸»å‡½æ•°: è¯„ä¼°å¹¶å¯è§†åŒ–è®­ç»ƒç»“æœ"""

    # ==================== é…ç½®è·¯å¾„ ====================
    # è¯·æ ¹æ®æ‚¨çš„å®é™…è®­ç»ƒè¾“å‡ºè·¯å¾„ä¿®æ”¹
    LOG_DIR = "./logs/ppo_fixed"
    MODEL_PATH = f"{LOG_DIR}/best_model"
    VEC_NORMALIZE_PATH = f"{LOG_DIR}/vec_normalize.pkl"

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(MODEL_PATH + ".zip"):
        print(f"âŒ Model not found: {MODEL_PATH}.zip")
        print("Please update MODEL_PATH in the script!")
        return

    # ==================== åˆ›å»ºå¯è§†åŒ–å™¨ ====================
    visualizer = PPOVisualizer(
        model_path=MODEL_PATH,
        vec_normalize_path=VEC_NORMALIZE_PATH
    )

    # ==================== è¿è¡Œè¯„ä¼° ====================
    history = visualizer.evaluate_episode(seed=42)

    # ==================== ç”Ÿæˆå¯è§†åŒ– ====================
    print("\nGenerating visualizations...")

    # 1. ç»¼åˆåˆ†æå›¾ (8ä¸ªå­å›¾)
    visualizer.plot_comprehensive_analysis(
        history,
        save_path='ppo_comprehensive_analysis_fixed.png'
    )

    # 2. è¯¯å·®åˆ†è§£è¯¦ç»†å›¾ (4ä¸ªAgentåˆ†åˆ«æ˜¾ç¤º)
    visualizer.plot_error_decomposition(
        history,
        save_path='ppo_error_decomposition_fixed.png'
    )

    # 3. ä¿¯è§†å›¾å¯¹æ¯”
    visualizer.plot_top_view_comparison(
        history,
        save_path='ppo_topview_comparison_fixed.png'
    )

    print("\n" + "=" * 70)
    print("âœ… All visualizations complete!")
    print("=" * 70)
    print("Generated files:")
    print("  - ppo_comprehensive_analysis_fixed.png  (8-panel analysis)")
    print("  - ppo_error_decomposition_fixed.png     (error breakdown per agent)")
    print("  - ppo_topview_comparison_fixed.png      (trajectory comparison)")
    print("=" * 70)


if __name__ == "__main__":
    main()
