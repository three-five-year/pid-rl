# rl_env_f16_formation.py - å®Œå…¨ä¿®å¤ç‰ˆ

import numpy as np
import gymnasium as gym
from gymnasium import spaces
from typing import Dict, Tuple
from f16Model import F16
from pid_tracker import PIDTracker
from adaptive_negotiation_trajectory import AdaptiveNegotiationTrajectory


class FormationEnvFixed(gym.Env):
    """
    å®Œå…¨ä¿®å¤ç‰ˆç¯å¢ƒ:
    1. å¥–åŠ±å‡½æ•°å½’ä¸€åŒ–ï¼Œé™åˆ¶å•æ­¥èŒƒå›´åœ¨[-10, +5]
    2. åˆ†ç¦»æ°´å¹³/é«˜åº¦è¯¯å·®ï¼Œç‹¬ç«‹æƒ©ç½š
    3. åˆå§‹æ¡ä»¶ä¸main.pyå®Œå…¨ä¸€è‡´
    4. è®°å½•ä¸‰ç±»è¯¯å·®ç”¨äºå¯è§†åŒ–
    """

    def __init__(self, config: Dict):
        super().__init__()

        self.N = 4
        self.dt = config.get('dt', 0.05)
        self.max_steps = config.get('max_steps', 2400)

        # æ¿€æ´»å‚æ•°
        self.warmstart_steps = config.get('warmstart_steps', 1200)
        self.rl_threshold = config.get('rl_threshold', 150.0)
        self.distance_safety_margin = config.get('distance_safety_margin', 180.0)

        # ğŸ”¥ ä¿®å¤1: åˆ†ç¦»æ°´å¹³/é«˜åº¦æƒé‡
        self.w_track_h = config.get('w_track_h', 3.0)
        self.w_track_v = config.get('w_track_v', 2.0)
        self.w_safe = config.get('w_safe', 2.0)
        self.w_ctrl = config.get('w_ctrl', 0.05)
        self.w_smooth = config.get('w_smooth', 0.1)

        # å®‰å…¨å‚æ•°
        self.d_collision = 100.0
        self.d_danger = 160.0
        self.d_safe = 350.0

        # ğŸ”¥ ä¿®å¤2: å¢å¤§ç”µæ¢¯èˆµé¢é™å¹…
        self.delta_throttle_limit = config.get('delta_throttle_limit', 0.03)
        self.delta_elevator_limit = config.get('delta_elevator_limit', 5.0)
        self.delta_aileron_limit = config.get('delta_aileron_limit', 2.0)
        self.delta_rudder_limit = config.get('delta_rudder_limit', 2.0)

        # ğŸ”¥ ä¿®å¤3: æ ‡å‡†åˆå§‹åç§»é‡
        standard_offsets = config.get('standard_initial_offsets')
        if standard_offsets is not None:
            if isinstance(standard_offsets, list):
                self.standard_initial_offsets = np.array(standard_offsets)
            else:
                self.standard_initial_offsets = standard_offsets
        else:
            # é»˜è®¤å€¼ï¼ˆä¸main.pyä¸€è‡´ï¼‰
            self.standard_initial_offsets = np.array([
                [0.0, 0.0, 0.0],
                [-300.0, -150.0, 0.0],
                [-500.0, -500.0, 0.0],
                [-1000.0, 0.0, 0.0],
            ])

        # åŠ¨ä½œ/è§‚æµ‹ç©ºé—´
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(self.N * 4,), dtype=np.float32
        )
        obs_dim = 18 * self.N
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32
        )

        # ç¯å¢ƒç»„ä»¶
        self.agents = [F16(time_step=self.dt) for _ in range(self.N)]
        self.trackers = [
            PIDTracker(dt=self.dt, agent_role="leader"),
            PIDTracker(dt=self.dt, agent_role="follower_direct"),
            PIDTracker(dt=self.dt, agent_role="follower_direct"),
            PIDTracker(dt=self.dt, agent_role="follower_indirect")
        ]

        A = np.array([[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]])
        leader_access = np.array([1, 0, 0, 0])
        self.desired_offsets = [
            np.array([0.0, 0.0, 0.0]),
            np.array([-500.0, -500.0, 0.0]),
            np.array([-500.0, 500.0, 0.0]),
            np.array([-1000.0, 0.0, 0.0]),
        ]

        self.planner = AdaptiveNegotiationTrajectory(
            N=self.N, adjacency_matrix=A, leader_access=leader_access,
            formation_offsets=self.desired_offsets, k_gain=2.0,
            sensing_radius=350.0, safety_radius=100.0
        )

        self.leader_start_pos = np.array([1000.0, 0.0, -5000.0])
        self.leader_velocity = 350.0

        # çŠ¶æ€å˜é‡
        self.step_count = 0
        self.current_time = 0.0
        self.leader_pos = None
        self.leader_vel = None
        self.leader_heading = 0.0
        self.turn_rate = 0.0
        self.prev_actions = np.zeros(self.N * 4)
        self.rl_active = False

        # ğŸ”¥ ä¿®å¤4: æ‰©å±•ç»Ÿè®¡å˜é‡ï¼Œè®°å½•ä¸‰ç±»è¯¯å·®
        self.episode_stats = {
            'total_reward': 0.0,
            'min_distance_ever': float('inf'),
            'sum_tracking_error': 0.0,
            'sum_error_total': 0.0,  # Îµâ‚: çœŸå®è·Ÿè¸ªè¯¯å·®
            'sum_error_planning': 0.0,  # Îµâ‚‚: è½¨è¿¹é‡æ„è¯¯å·®
            'sum_error_tracking': 0.0,  # Îµâ‚ƒ: æ§åˆ¶å™¨è·Ÿè¸ªè¯¯å·®
            'sum_error_horizontal': 0.0,  # æ°´å¹³è¯¯å·®
            'sum_error_vertical': 0.0,  # é«˜åº¦è¯¯å·®
            'sum_rl_activation': 0,
            'step_count': 0,
            'collision': False,
            'max_tracking_error': 0.0
        }

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)

        self.step_count = 0
        self.current_time = 0.0
        self.leader_heading = 0.0
        self.turn_rate = 0.0
        self.prev_actions = np.zeros(self.N * 4)
        self.rl_active = False

        # é‡ç½®ç»Ÿè®¡
        self.episode_stats = {
            'total_reward': 0.0,
            'min_distance_ever': float('inf'),
            'sum_tracking_error': 0.0,
            'sum_error_total': 0.0,
            'sum_error_planning': 0.0,
            'sum_error_tracking': 0.0,
            'sum_error_horizontal': 0.0,
            'sum_error_vertical': 0.0,
            'sum_rl_activation': 0,
            'step_count': 0,
            'collision': False,
            'max_tracking_error': 0.0
        }

        # ğŸ”¥ ä¿®å¤3: ä½¿ç”¨æ ‡å‡†åˆå§‹ä½ç½®ï¼ˆä¸main.pyå®Œå…¨ä¸€è‡´ï¼‰
        initial_positions = self.leader_start_pos + self.standard_initial_offsets

        # âŒ ç§»é™¤éšæœºå™ªå£°ï¼ˆä¿è¯å…¬å¹³æ€§ï¼‰
        # noise = np.random.uniform(-30, 30, (self.N, 3))
        # noise[:, 2] = 0
        # initial_positions[1:] += noise[1:]

        self.planner.initialize(initial_positions)
        self.leader_pos = self.leader_start_pos.copy()
        self.leader_vel = np.array([self.leader_velocity, 0.0, 0.0])

        for i, agent in enumerate(self.agents):
            agent.reset(
                position=initial_positions[i],
                velocity_body=np.array([350.0, 0.0, 0.0]),
                mach=0.35
            )

        # ğŸ”¥ é‡ç½®PIDç§¯åˆ†å™¨
        for tracker in self.trackers:
            tracker.int_lat = 0.0
            tracker.int_v = 0.0
            tracker.alpha_int = 0.0

        obs = self._get_observation()
        return obs, {}

    def step(self, action: np.ndarray):
        self.step_count += 1
        self.current_time = self.step_count * self.dt

        # Warm-Starté˜¶æ®µ
        if self.step_count <= self.warmstart_steps:
            return self._pure_pid_step()

        # æ›´æ–°é¢†æœºè½¨è¿¹
        self._update_leader_trajectory()

        c, s = np.cos(self.leader_heading), np.sin(self.leader_heading)
        R_z = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])
        current_offsets = [R_z @ off for off in self.desired_offsets]
        self.planner.update_offsets(current_offsets)

        is_turning = abs(self.turn_rate) > 1e-3
        if is_turning:
            for _ in range(3):
                self.planner.step(self.leader_pos, self.leader_vel, self.dt / 3)
        else:
            for _ in range(8):
                self.planner.step(self.leader_pos, self.leader_vel, self.dt / 8)

        target_pos_all, target_vel_all = self.planner.get_target_trajectories()

        # è®¡ç®—å½“å‰çŠ¶æ€
        current_positions = np.array([agent.position for agent in self.agents])

        # ğŸ”¥ ä¿®å¤4: è®¡ç®—ä¸‰ç±»è¯¯å·®
        error_total_list = []
        error_planning_list = []
        error_tracking_list = []
        error_horizontal_list = []
        error_vertical_list = []

        for i in range(self.N):
            # ç†æƒ³ç¼–é˜Ÿä½ç½®
            ideal_formation = self.leader_pos + current_offsets[i]

            # Îµâ‚: çœŸå®è·Ÿè¸ªè¯¯å·® (Agent vs ç†æƒ³ç¼–é˜Ÿ)
            e_total = current_positions[i] - ideal_formation
            error_total = np.linalg.norm(e_total)
            error_total_list.append(error_total)

            # æ°´å¹³/é«˜åº¦åˆ†è§£
            error_horizontal = np.linalg.norm(e_total[0:2])
            error_vertical = abs(e_total[2])
            error_horizontal_list.append(error_horizontal)
            error_vertical_list.append(error_vertical)

            # Îµâ‚‚: è½¨è¿¹é‡æ„è¯¯å·® (åå•†è½¨è¿¹ vs ç†æƒ³ç¼–é˜Ÿ)
            error_planning = np.linalg.norm(target_pos_all[i] - ideal_formation)
            error_planning_list.append(error_planning)

            # Îµâ‚ƒ: æ§åˆ¶å™¨è·Ÿè¸ªè¯¯å·® (Agent vs åå•†è½¨è¿¹)
            error_tracking = np.linalg.norm(current_positions[i] - target_pos_all[i])
            error_tracking_list.append(error_tracking)

        avg_error_total = np.mean(error_total_list)
        avg_error_horizontal = np.mean(error_horizontal_list)
        avg_error_vertical = np.mean(error_vertical_list)
        max_track_err = max(error_tracking_list)

        # ğŸ”¥ ä¿®å¤: æ­£ç¡®è®¡ç®—æœ€å°è·ç¦»
        min_dist = float('inf')
        for i in range(self.N):
            for j in range(i + 1, self.N):
                d = np.linalg.norm(current_positions[i] - current_positions[j])
                if d > 1.0:
                    min_dist = min(min_dist, d)

        if min_dist == float('inf'):
            min_dist = 500.0

        # RLæ¿€æ´»é€»è¾‘
        self.rl_active = (
                max_track_err > self.rl_threshold and
                min_dist > self.distance_safety_margin
        )

        # è§£æåŠ¨ä½œ
        action = np.clip(action, -1.0, 1.0)
        delta_u_all = np.zeros((self.N, 4))

        if self.rl_active:
            for i in range(self.N):
                delta_u_all[i] = np.array([
                    action[i * 4 + 0] * self.delta_throttle_limit,
                    action[i * 4 + 1] * self.delta_elevator_limit,
                    action[i * 4 + 2] * self.delta_aileron_limit,
                    action[i * 4 + 3] * self.delta_rudder_limit
                ])

        # æ‰§è¡Œæ§åˆ¶
        for i in range(self.N):
            u_pid = self.trackers[i].compute_control(
                target_pos=target_pos_all[i],
                target_vel=target_vel_all[i],
                current_pos=self.agents[i].position,
                vel_earth=self.agents[i].velocity_earth,
                euler_rad=self.agents[i].euler,
                alpha_rad=self.agents[i].alpha,
                beta_rad=self.agents[i].beta,
                rot_body2earth=self.agents[i].rotation_body2earth,
                feedforward_turn_rate=self.turn_rate,
                leader_pos=self.leader_pos
            )

            u_total = u_pid + delta_u_all[i]

            u_total[0] = np.clip(u_total[0], 0.0, 1.0)
            u_total[1] = np.clip(u_total[1], -25.0, 25.0)
            u_total[2] = np.clip(u_total[2], -21.5, 21.5)
            u_total[3] = np.clip(u_total[3], -30.0, 30.0)

            self.agents[i].step(u_total)

        # ğŸ”¥ ä¿®å¤1: æ–°çš„å¥–åŠ±è®¡ç®—ï¼ˆåˆ†ç¦»æ°´å¹³/é«˜åº¦ï¼Œé™åˆ¶èŒƒå›´ï¼‰
        reward, reward_info = self._compute_reward_fixed(
            avg_error_horizontal, avg_error_vertical, min_dist, action
        )

        # ç»ˆæ­¢æ¡ä»¶
        terminated = min_dist < self.d_collision
        truncated = self.step_count >= self.max_steps

        if terminated:
            self.episode_stats['collision'] = True

        # ç»Ÿè®¡ç´¯åŠ 
        self.episode_stats['total_reward'] += reward
        self.episode_stats['min_distance_ever'] = min(
            self.episode_stats['min_distance_ever'], min_dist
        )
        self.episode_stats['sum_tracking_error'] += avg_error_total
        self.episode_stats['sum_error_total'] += avg_error_total
        self.episode_stats['sum_error_planning'] += np.mean(error_planning_list)
        self.episode_stats['sum_error_tracking'] += np.mean(error_tracking_list)
        self.episode_stats['sum_error_horizontal'] += avg_error_horizontal
        self.episode_stats['sum_error_vertical'] += avg_error_vertical
        self.episode_stats['sum_rl_activation'] += (1 if self.rl_active else 0)
        self.episode_stats['max_tracking_error'] = max(
            self.episode_stats['max_tracking_error'], max_track_err
        )
        self.episode_stats['step_count'] += 1

        # åœ¨episodeç»“æŸæ—¶è®¡ç®—å¹³å‡å€¼
        final_stats = {}
        if terminated or truncated:
            n_steps = self.episode_stats['step_count']
            if n_steps > 0:
                final_stats = {
                    'total_reward': self.episode_stats['total_reward'],
                    'min_distance_ever': self.episode_stats['min_distance_ever'],
                    'avg_tracking_error': self.episode_stats['sum_tracking_error'] / n_steps,
                    'avg_error_total': self.episode_stats['sum_error_total'] / n_steps,
                    'avg_error_planning': self.episode_stats['sum_error_planning'] / n_steps,
                    'avg_error_tracking': self.episode_stats['sum_error_tracking'] / n_steps,
                    'avg_error_horizontal': self.episode_stats['sum_error_horizontal'] / n_steps,
                    'avg_error_vertical': self.episode_stats['sum_error_vertical'] / n_steps,
                    'max_tracking_error': self.episode_stats['max_tracking_error'],
                    'rl_activation_ratio': self.episode_stats['sum_rl_activation'] / n_steps,
                    'collision': self.episode_stats['collision']
                }

        obs = self._get_observation()
        info = {
            **reward_info,
            'rl_active': self.rl_active,
            'collision': terminated,
            'episode_stats': final_stats,
            # ğŸ”¥ æ·»åŠ ä¸‰ç±»è¯¯å·®åˆ°info
            'error_total': avg_error_total,
            'error_horizontal': avg_error_horizontal,
            'error_vertical': avg_error_vertical,
        }

        self.prev_actions = action.copy()

        return obs, reward, terminated, truncated, info

    def _pure_pid_step(self):
        """Warm-Starté˜¶æ®µ"""
        self._update_leader_trajectory()

        c, s = np.cos(self.leader_heading), np.sin(self.leader_heading)
        R_z = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])
        current_offsets = [R_z @ off for off in self.desired_offsets]
        self.planner.update_offsets(current_offsets)

        is_turning = abs(self.turn_rate) > 1e-3
        if is_turning:
            for _ in range(3):
                self.planner.step(self.leader_pos, self.leader_vel, self.dt / 3)
        else:
            for _ in range(8):
                self.planner.step(self.leader_pos, self.leader_vel, self.dt / 8)

        target_pos_all, target_vel_all = self.planner.get_target_trajectories()

        for i in range(self.N):
            u_pid = self.trackers[i].compute_control(
                target_pos=target_pos_all[i],
                target_vel=target_vel_all[i],
                current_pos=self.agents[i].position,
                vel_earth=self.agents[i].velocity_earth,
                euler_rad=self.agents[i].euler,
                alpha_rad=self.agents[i].alpha,
                beta_rad=self.agents[i].beta,
                rot_body2earth=self.agents[i].rotation_body2earth,
                feedforward_turn_rate=self.turn_rate,
                leader_pos=self.leader_pos
            )
            self.agents[i].step(u_pid)

        obs = self._get_observation()
        return obs, 0.0, False, self.step_count >= self.max_steps, {'warmstart': True}

    def _compute_reward_fixed(self, avg_error_h, avg_error_v, min_dist, action):
        """
        ğŸ”¥ ä¿®å¤ç‰ˆå¥–åŠ±å‡½æ•°:
        1. åˆ†ç¦»æ°´å¹³/é«˜åº¦è¯¯å·®
        2. é™åˆ¶å•æ­¥å¥–åŠ±èŒƒå›´åœ¨[-10, +5]
        3. ä½¿ç”¨clipè€Œétanhä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±
        """

        # 1. æ°´å¹³è·Ÿè¸ªå¥–åŠ±: [-1, 0] Ã— w_track_h(3.0) = [-3, 0]
        r_track_h_raw = -np.clip(avg_error_h / 100.0, 0.0, 1.0)
        r_track_h = r_track_h_raw * self.w_track_h

        # 2. é«˜åº¦è·Ÿè¸ªå¥–åŠ±: [-1, 0] Ã— w_track_v(2.0) = [-2, 0]
        r_track_v_raw = -np.clip(avg_error_v / 50.0, 0.0, 1.0)
        r_track_v = r_track_v_raw * self.w_track_v

        # 3. å®‰å…¨å¥–åŠ±: [-1, +0.2] Ã— w_safe(2.0) = [-2, 0.4]
        if min_dist < self.d_collision:
            r_safe_raw = -1.0
        elif min_dist < self.d_danger:
            alpha = (min_dist - self.d_collision) / (self.d_danger - self.d_collision)
            r_safe_raw = -1.0 + alpha * 0.5
        elif min_dist < self.d_safe:
            alpha = (min_dist - self.d_danger) / (self.d_safe - self.d_danger)
            r_safe_raw = -0.5 + alpha * 0.5
        else:
            bonus = min(1.0, (min_dist - self.d_safe) / 200.0)
            r_safe_raw = bonus * 0.2

        r_safe = np.clip(r_safe_raw, -1.0, 0.2) * self.w_safe

        # 4. æ§åˆ¶æƒ©ç½š: [-1, 0] Ã— w_ctrl(0.05) = [-0.05, 0]
        if self.rl_active:
            action_norm = np.linalg.norm(action) / np.sqrt(self.N * 4)
            r_ctrl = -np.clip(action_norm, 0.0, 1.0) * self.w_ctrl

            action_change = np.linalg.norm(action - self.prev_actions) / np.sqrt(self.N * 4)
            r_smooth = -np.clip(action_change, 0.0, 1.0) * self.w_smooth
        else:
            r_ctrl = 0.0
            r_smooth = 0.0

        # 5. Bonus: [0, 1.5]
        r_bonus = 0.0
        if avg_error_h < 50.0 and avg_error_v < 25.0 and min_dist > 300.0:
            r_bonus += 0.5
        if self.step_count >= self.max_steps - 10 and min_dist > 200.0:
            r_bonus += 1.0

        # æ€»å¥–åŠ±: ç†è®ºèŒƒå›´ [-7.05, 1.9]
        reward = r_track_h + r_track_v + r_safe + r_ctrl + r_smooth + r_bonus

        # ğŸ”¥ é¢å¤–ä¿æŠ¤ï¼šclipåˆ°[-10, +5]
        reward = np.clip(reward, -10.0, 5.0)

        reward_info = {
            'r_track_h': r_track_h,
            'r_track_v': r_track_v,
            'r_safe': r_safe,
            'r_ctrl': r_ctrl,
            'r_smooth': r_smooth,
            'r_bonus': r_bonus,
            'avg_error_h': avg_error_h,
            'avg_error_v': avg_error_v,
            'min_distance': min_dist
        }

        return reward, reward_info

    def _update_leader_trajectory(self):
        """é¢†æœºè½¨è¿¹(åŒ…å«90Â°è½¬å¼¯)"""
        t = self.current_time
        turn_start = 20.0
        turn_end = 70.0
        transition_time = 5.0
        turn_rate_max = np.deg2rad(90.0 / 50.0)

        if t < turn_start:
            omega = 0.0
        elif t < turn_start + transition_time:
            progress = (t - turn_start) / transition_time
            smooth = 3 * progress ** 2 - 2 * progress ** 3
            omega = turn_rate_max * smooth
        elif t < turn_end - transition_time:
            omega = turn_rate_max
        elif t < turn_end:
            progress = (turn_end - t) / transition_time
            smooth = 3 * progress ** 2 - 2 * progress ** 3
            omega = turn_rate_max * smooth
        else:
            omega = 0.0

        self.turn_rate = omega
        self.leader_heading += omega * self.dt

        vel = np.array([
            self.leader_velocity * np.cos(self.leader_heading),
            self.leader_velocity * np.sin(self.leader_heading),
            0.0
        ])

        self.leader_pos += vel * self.dt
        self.leader_vel = vel

    def _get_observation(self):
        """è§‚æµ‹"""
        obs = []

        c, s = np.cos(self.leader_heading), np.sin(self.leader_heading)
        R_z = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])
        current_offsets = [R_z @ off for off in self.desired_offsets]

        for i in range(self.N):
            agent = self.agents[i]
            ref_pos = self.leader_pos + current_offsets[i]
            ref_vel = self.leader_vel

            e_p = ref_pos - agent.position
            e_v = ref_vel - agent.velocity_earth

            leader_relative = agent.position - self.agents[0].position
            desired_relative = current_offsets[i] - current_offsets[0]
            e_form = leader_relative - desired_relative

            euler = agent.euler
            pqr = agent.angular_velocity

            min_d = float('inf')
            for j in range(self.N):
                if j != i:
                    d = np.linalg.norm(agent.position - self.agents[j].position)
                    if d > 1.0:
                        min_d = min(min_d, d)

            if min_d == float('inf'):
                min_d = 500.0

            danger_flag = 1.0 if min_d < self.d_danger else 0.0

            agent_obs = np.concatenate([
                e_p / 1000.0,
                e_v / 100.0,
                e_form / 500.0,
                euler,
                pqr,
                [min_d / 1000.0],
                [danger_flag],
                [self.turn_rate]
            ])

            obs.append(agent_obs)

        return np.concatenate(obs).astype(np.float32)